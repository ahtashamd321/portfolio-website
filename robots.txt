# robots.txt for Data Analyst Portfolio

# Allow all search engines to crawl the entire site
User-agent: *
Allow: /

# Disallow crawling of any admin or private directories (if you add them later)
# Disallow: /admin/
# Disallow: /private/

# Sitemap location (update with your actual URL)
Sitemap: https://yourusername.github.io/sitemap.xml

# Crawl delay (optional - helps prevent server overload)
# Crawl-delay: 10